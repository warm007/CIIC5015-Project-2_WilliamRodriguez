{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSODimh2+fF2COCAiSTZqv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/warm007/CIIC5015-Project-2_WilliamRodriguez/blob/main/P2_Classification_William_Rodriguez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# William A. Rodríguez Mercado\n",
        "# CIIC 5015 Sec 096\n",
        "\n",
        "# **Classification**"
      ],
      "metadata": {
        "id": "YrpHBNImfChq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fzdg_AGLPAMK"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "#Set the amount of desires epochs, input size and learning rate\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "Q0eJXHMsPKM3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Template\n",
        "writer1 = SummaryWriter('runs/Template')\n",
        "\n",
        "# For the second model\n",
        "writer2 = SummaryWriter('runs/Network1')\n",
        "\n",
        "# For the third model\n",
        "writer3 = SummaryWriter('runs/Network2')\n",
        "\n",
        "writer4 = SummaryWriter('runs/Network3')"
      ],
      "metadata": {
        "id": "BXZ2CnVpJpN-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Template: 3-layer implementation of a network**"
      ],
      "metadata": {
        "id": "jkYOchS3LUoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully connected neural network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uUUGdJ4WPuIY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network #1: 4-layer network**\n",
        "\n",
        "\n",
        "*   Layer 1 – 20 neurons\n",
        "*  Layer 2 – 50 neurons\n",
        "*   Layer 3 – 20 neurons\n",
        "*   Layer 4 – output neuron with softmax activation\n",
        "\n"
      ],
      "metadata": {
        "id": "5fP5v_P9LgKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network1(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Network1, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 20)\n",
        "        self.fc2 = nn.Linear(20, 50)\n",
        "        self.fc3 = nn.Linear(50, 20)\n",
        "        self.fc4 = nn.Linear(20, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EpMgvLmtLtE-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network #2: 6- layer network**\n",
        "\n",
        "*   Layer 1 – 10 neurons\n",
        "*   Layer 2 – 20 neurons\n",
        "*   Layer 3 – 30 neurons\n",
        "*   Layer 4 – 20 neurons\n",
        "*   Layer 5 – 10 neurons\n",
        "*   Layer 6 – output neuron with softmax activation\n"
      ],
      "metadata": {
        "id": "5ZpnGkzsL6-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network2(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Network2, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 10)\n",
        "        self.fc2 = nn.Linear(10, 20)\n",
        "        self.fc3 = nn.Linear(20, 30)\n",
        "        self.fc4 = nn.Linear(30, 20)\n",
        "        self.fc5 = nn.Linear(20, 10)\n",
        "        self.fc6 = nn.Linear(10, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x  # Apply softmax on the output"
      ],
      "metadata": {
        "id": "v5oDNv7PL_uA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network #3: 6- layer network**\n",
        "\n",
        "*   Layer 1 – 10 neurons\n",
        "*   Layer 2 – 40 neurons\n",
        "*   Layer 3 – 70 neurons\n",
        "*   Layer 4 – 40 neurons\n",
        "*   Layer 5 – 10 neurons\n",
        "*   Layer 6 – output neuron with softmax activation\n",
        "\n"
      ],
      "metadata": {
        "id": "J6DqnqA8M1Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network3(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(Network3, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 10)\n",
        "        self.fc2 = nn.Linear(10, 40)\n",
        "        self.fc3 = nn.Linear(40, 70)\n",
        "        self.fc4 = nn.Linear(70, 40)\n",
        "        self.fc5 = nn.Linear(40, 10)\n",
        "        self.fc6 = nn.Linear(10, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CEPvkYiqNi2Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init Models:"
      ],
      "metadata": {
        "id": "Jt7BEmLOPA7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Network1Model = Network1(input_size,num_classes).to(device)\n",
        "\n",
        "Network2Model = Network2(input_size,num_classes)\n",
        "\n",
        "Network3Model = Network3(input_size,num_classes)\n",
        "\n"
      ],
      "metadata": {
        "id": "JhVFUAqPPIu2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Training and Testing Functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nqq3DJH4P_5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(model, criterion, optimizer, train_loader, device, num_epochs, writer):\n",
        "    total_step = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss,count = 0, 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Move tensors to the configured device\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            writer.add_scalar('Loss/train', loss.item(), epoch )\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "        writer.add_scalar('Loss/train', total_loss/count, epoch + 1 )\n",
        ""
      ],
      "metadata": {
        "id": "H-3RQrhPPwu_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "def test_model(model, test_loader, device, writer, epoch):\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print('Accuracy of the network on the 10000 test images: {} %'.format(accuracy))\n",
        "        writer.add_scalar('accuracy', accuracy, epoch)"
      ],
      "metadata": {
        "id": "eYgfnoVGR1q_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing Results Network1"
      ],
      "metadata": {
        "id": "ZCFeMFRdRu9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer1 = torch.optim.Adam(Network1Model.parameters(), lr=learning_rate)\n",
        "train_model(Network1Model, criterion, optimizer1, train_loader, device, num_epochs, writer2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op0jkuahO7UW",
        "outputId": "2b27ceb2-4f1a-4979-91e8-cf772220865a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [100/600], Loss: 0.4792\n",
            "Epoch [1/20], Step [200/600], Loss: 0.1667\n",
            "Epoch [1/20], Step [300/600], Loss: 0.4245\n",
            "Epoch [1/20], Step [400/600], Loss: 0.2301\n",
            "Epoch [1/20], Step [500/600], Loss: 0.2292\n",
            "Epoch [1/20], Step [600/600], Loss: 0.2453\n",
            "Epoch [2/20], Step [100/600], Loss: 0.1446\n",
            "Epoch [2/20], Step [200/600], Loss: 0.1617\n",
            "Epoch [2/20], Step [300/600], Loss: 0.1406\n",
            "Epoch [2/20], Step [400/600], Loss: 0.2659\n",
            "Epoch [2/20], Step [500/600], Loss: 0.2897\n",
            "Epoch [2/20], Step [600/600], Loss: 0.1376\n",
            "Epoch [3/20], Step [100/600], Loss: 0.1916\n",
            "Epoch [3/20], Step [200/600], Loss: 0.0366\n",
            "Epoch [3/20], Step [300/600], Loss: 0.2053\n",
            "Epoch [3/20], Step [400/600], Loss: 0.1807\n",
            "Epoch [3/20], Step [500/600], Loss: 0.0917\n",
            "Epoch [3/20], Step [600/600], Loss: 0.2140\n",
            "Epoch [4/20], Step [100/600], Loss: 0.1223\n",
            "Epoch [4/20], Step [200/600], Loss: 0.3844\n",
            "Epoch [4/20], Step [300/600], Loss: 0.1767\n",
            "Epoch [4/20], Step [400/600], Loss: 0.0737\n",
            "Epoch [4/20], Step [500/600], Loss: 0.1029\n",
            "Epoch [4/20], Step [600/600], Loss: 0.0456\n",
            "Epoch [5/20], Step [100/600], Loss: 0.0845\n",
            "Epoch [5/20], Step [200/600], Loss: 0.1161\n",
            "Epoch [5/20], Step [300/600], Loss: 0.2758\n",
            "Epoch [5/20], Step [400/600], Loss: 0.5292\n",
            "Epoch [5/20], Step [500/600], Loss: 0.1171\n",
            "Epoch [5/20], Step [600/600], Loss: 0.0889\n",
            "Epoch [6/20], Step [100/600], Loss: 0.0352\n",
            "Epoch [6/20], Step [200/600], Loss: 0.1274\n",
            "Epoch [6/20], Step [300/600], Loss: 0.2725\n",
            "Epoch [6/20], Step [400/600], Loss: 0.2134\n",
            "Epoch [6/20], Step [500/600], Loss: 0.1291\n",
            "Epoch [6/20], Step [600/600], Loss: 0.1091\n",
            "Epoch [7/20], Step [100/600], Loss: 0.2257\n",
            "Epoch [7/20], Step [200/600], Loss: 0.2155\n",
            "Epoch [7/20], Step [300/600], Loss: 0.1941\n",
            "Epoch [7/20], Step [400/600], Loss: 0.1887\n",
            "Epoch [7/20], Step [500/600], Loss: 0.1251\n",
            "Epoch [7/20], Step [600/600], Loss: 0.2238\n",
            "Epoch [8/20], Step [100/600], Loss: 0.2309\n",
            "Epoch [8/20], Step [200/600], Loss: 0.2025\n",
            "Epoch [8/20], Step [300/600], Loss: 0.1924\n",
            "Epoch [8/20], Step [400/600], Loss: 0.0976\n",
            "Epoch [8/20], Step [500/600], Loss: 0.1663\n",
            "Epoch [8/20], Step [600/600], Loss: 0.1378\n",
            "Epoch [9/20], Step [100/600], Loss: 0.1646\n",
            "Epoch [9/20], Step [200/600], Loss: 0.1141\n",
            "Epoch [9/20], Step [300/600], Loss: 0.1914\n",
            "Epoch [9/20], Step [400/600], Loss: 0.0997\n",
            "Epoch [9/20], Step [500/600], Loss: 0.1577\n",
            "Epoch [9/20], Step [600/600], Loss: 0.0845\n",
            "Epoch [10/20], Step [100/600], Loss: 0.0563\n",
            "Epoch [10/20], Step [200/600], Loss: 0.0836\n",
            "Epoch [10/20], Step [300/600], Loss: 0.1475\n",
            "Epoch [10/20], Step [400/600], Loss: 0.0917\n",
            "Epoch [10/20], Step [500/600], Loss: 0.1401\n",
            "Epoch [10/20], Step [600/600], Loss: 0.0917\n",
            "Epoch [11/20], Step [100/600], Loss: 0.1590\n",
            "Epoch [11/20], Step [200/600], Loss: 0.0961\n",
            "Epoch [11/20], Step [300/600], Loss: 0.1631\n",
            "Epoch [11/20], Step [400/600], Loss: 0.1151\n",
            "Epoch [11/20], Step [500/600], Loss: 0.1553\n",
            "Epoch [11/20], Step [600/600], Loss: 0.1560\n",
            "Epoch [12/20], Step [100/600], Loss: 0.1086\n",
            "Epoch [12/20], Step [200/600], Loss: 0.1720\n",
            "Epoch [12/20], Step [300/600], Loss: 0.1816\n",
            "Epoch [12/20], Step [400/600], Loss: 0.0694\n",
            "Epoch [12/20], Step [500/600], Loss: 0.0838\n",
            "Epoch [12/20], Step [600/600], Loss: 0.1013\n",
            "Epoch [13/20], Step [100/600], Loss: 0.0240\n",
            "Epoch [13/20], Step [200/600], Loss: 0.1749\n",
            "Epoch [13/20], Step [300/600], Loss: 0.0966\n",
            "Epoch [13/20], Step [400/600], Loss: 0.1135\n",
            "Epoch [13/20], Step [500/600], Loss: 0.0896\n",
            "Epoch [13/20], Step [600/600], Loss: 0.1125\n",
            "Epoch [14/20], Step [100/600], Loss: 0.1734\n",
            "Epoch [14/20], Step [200/600], Loss: 0.1065\n",
            "Epoch [14/20], Step [300/600], Loss: 0.0725\n",
            "Epoch [14/20], Step [400/600], Loss: 0.0482\n",
            "Epoch [14/20], Step [500/600], Loss: 0.1423\n",
            "Epoch [14/20], Step [600/600], Loss: 0.1439\n",
            "Epoch [15/20], Step [100/600], Loss: 0.1350\n",
            "Epoch [15/20], Step [200/600], Loss: 0.1311\n",
            "Epoch [15/20], Step [300/600], Loss: 0.1425\n",
            "Epoch [15/20], Step [400/600], Loss: 0.1004\n",
            "Epoch [15/20], Step [500/600], Loss: 0.1710\n",
            "Epoch [15/20], Step [600/600], Loss: 0.1190\n",
            "Epoch [16/20], Step [100/600], Loss: 0.0538\n",
            "Epoch [16/20], Step [200/600], Loss: 0.0741\n",
            "Epoch [16/20], Step [300/600], Loss: 0.1187\n",
            "Epoch [16/20], Step [400/600], Loss: 0.0878\n",
            "Epoch [16/20], Step [500/600], Loss: 0.1184\n",
            "Epoch [16/20], Step [600/600], Loss: 0.1009\n",
            "Epoch [17/20], Step [100/600], Loss: 0.2141\n",
            "Epoch [17/20], Step [200/600], Loss: 0.0348\n",
            "Epoch [17/20], Step [300/600], Loss: 0.1107\n",
            "Epoch [17/20], Step [400/600], Loss: 0.0520\n",
            "Epoch [17/20], Step [500/600], Loss: 0.2013\n",
            "Epoch [17/20], Step [600/600], Loss: 0.1602\n",
            "Epoch [18/20], Step [100/600], Loss: 0.1511\n",
            "Epoch [18/20], Step [200/600], Loss: 0.0544\n",
            "Epoch [18/20], Step [300/600], Loss: 0.0724\n",
            "Epoch [18/20], Step [400/600], Loss: 0.0746\n",
            "Epoch [18/20], Step [500/600], Loss: 0.0956\n",
            "Epoch [18/20], Step [600/600], Loss: 0.0548\n",
            "Epoch [19/20], Step [100/600], Loss: 0.1502\n",
            "Epoch [19/20], Step [200/600], Loss: 0.0833\n",
            "Epoch [19/20], Step [300/600], Loss: 0.1043\n",
            "Epoch [19/20], Step [400/600], Loss: 0.0483\n",
            "Epoch [19/20], Step [500/600], Loss: 0.1029\n",
            "Epoch [19/20], Step [600/600], Loss: 0.1678\n",
            "Epoch [20/20], Step [100/600], Loss: 0.1294\n",
            "Epoch [20/20], Step [200/600], Loss: 0.1147\n",
            "Epoch [20/20], Step [300/600], Loss: 0.0809\n",
            "Epoch [20/20], Step [400/600], Loss: 0.0627\n",
            "Epoch [20/20], Step [500/600], Loss: 0.0651\n",
            "Epoch [20/20], Step [600/600], Loss: 0.0932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(Network1Model, test_loader, device, writer2, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgLjPXJ0Sgta",
        "outputId": "f1a91bba-0b97-4ed9-b466-c246a99e72b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 95.83 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing Results Network2"
      ],
      "metadata": {
        "id": "li8LTxd-SsPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer2 = torch.optim.Adam(Network2Model.parameters(), lr=learning_rate)\n",
        "train_model(Network2Model, criterion, optimizer2, train_loader, device, num_epochs, writer3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIojUbhNSv-W",
        "outputId": "f580ef2e-8b3b-4f09-8f2f-f6ab4a297cc3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [100/600], Loss: 0.9902\n",
            "Epoch [1/20], Step [200/600], Loss: 0.5511\n",
            "Epoch [1/20], Step [300/600], Loss: 0.3832\n",
            "Epoch [1/20], Step [400/600], Loss: 0.8618\n",
            "Epoch [1/20], Step [500/600], Loss: 0.3482\n",
            "Epoch [1/20], Step [600/600], Loss: 0.7320\n",
            "Epoch [2/20], Step [100/600], Loss: 0.5134\n",
            "Epoch [2/20], Step [200/600], Loss: 0.3247\n",
            "Epoch [2/20], Step [300/600], Loss: 0.4299\n",
            "Epoch [2/20], Step [400/600], Loss: 0.3082\n",
            "Epoch [2/20], Step [500/600], Loss: 0.2593\n",
            "Epoch [2/20], Step [600/600], Loss: 0.3746\n",
            "Epoch [3/20], Step [100/600], Loss: 0.4856\n",
            "Epoch [3/20], Step [200/600], Loss: 0.3953\n",
            "Epoch [3/20], Step [300/600], Loss: 0.3570\n",
            "Epoch [3/20], Step [400/600], Loss: 0.3695\n",
            "Epoch [3/20], Step [500/600], Loss: 0.3032\n",
            "Epoch [3/20], Step [600/600], Loss: 0.3092\n",
            "Epoch [4/20], Step [100/600], Loss: 0.3542\n",
            "Epoch [4/20], Step [200/600], Loss: 0.3031\n",
            "Epoch [4/20], Step [300/600], Loss: 0.3831\n",
            "Epoch [4/20], Step [400/600], Loss: 0.4153\n",
            "Epoch [4/20], Step [500/600], Loss: 0.2011\n",
            "Epoch [4/20], Step [600/600], Loss: 0.2259\n",
            "Epoch [5/20], Step [100/600], Loss: 0.2568\n",
            "Epoch [5/20], Step [200/600], Loss: 0.2571\n",
            "Epoch [5/20], Step [300/600], Loss: 0.3632\n",
            "Epoch [5/20], Step [400/600], Loss: 0.3487\n",
            "Epoch [5/20], Step [500/600], Loss: 0.2610\n",
            "Epoch [5/20], Step [600/600], Loss: 0.3228\n",
            "Epoch [6/20], Step [100/600], Loss: 0.1912\n",
            "Epoch [6/20], Step [200/600], Loss: 0.2856\n",
            "Epoch [6/20], Step [300/600], Loss: 0.3442\n",
            "Epoch [6/20], Step [400/600], Loss: 0.3013\n",
            "Epoch [6/20], Step [500/600], Loss: 0.3939\n",
            "Epoch [6/20], Step [600/600], Loss: 0.3471\n",
            "Epoch [7/20], Step [100/600], Loss: 0.2147\n",
            "Epoch [7/20], Step [200/600], Loss: 0.3730\n",
            "Epoch [7/20], Step [300/600], Loss: 0.4334\n",
            "Epoch [7/20], Step [400/600], Loss: 0.3195\n",
            "Epoch [7/20], Step [500/600], Loss: 0.2783\n",
            "Epoch [7/20], Step [600/600], Loss: 0.2927\n",
            "Epoch [8/20], Step [100/600], Loss: 0.1363\n",
            "Epoch [8/20], Step [200/600], Loss: 0.4059\n",
            "Epoch [8/20], Step [300/600], Loss: 0.2841\n",
            "Epoch [8/20], Step [400/600], Loss: 0.2924\n",
            "Epoch [8/20], Step [500/600], Loss: 0.1732\n",
            "Epoch [8/20], Step [600/600], Loss: 0.2381\n",
            "Epoch [9/20], Step [100/600], Loss: 0.2601\n",
            "Epoch [9/20], Step [200/600], Loss: 0.1653\n",
            "Epoch [9/20], Step [300/600], Loss: 0.2335\n",
            "Epoch [9/20], Step [400/600], Loss: 0.3333\n",
            "Epoch [9/20], Step [500/600], Loss: 0.3051\n",
            "Epoch [9/20], Step [600/600], Loss: 0.5645\n",
            "Epoch [10/20], Step [100/600], Loss: 0.2580\n",
            "Epoch [10/20], Step [200/600], Loss: 0.1780\n",
            "Epoch [10/20], Step [300/600], Loss: 0.3658\n",
            "Epoch [10/20], Step [400/600], Loss: 0.1945\n",
            "Epoch [10/20], Step [500/600], Loss: 0.4339\n",
            "Epoch [10/20], Step [600/600], Loss: 0.2564\n",
            "Epoch [11/20], Step [100/600], Loss: 0.3471\n",
            "Epoch [11/20], Step [200/600], Loss: 0.1609\n",
            "Epoch [11/20], Step [300/600], Loss: 0.2923\n",
            "Epoch [11/20], Step [400/600], Loss: 0.4691\n",
            "Epoch [11/20], Step [500/600], Loss: 0.1495\n",
            "Epoch [11/20], Step [600/600], Loss: 0.1650\n",
            "Epoch [12/20], Step [100/600], Loss: 0.2441\n",
            "Epoch [12/20], Step [200/600], Loss: 0.2537\n",
            "Epoch [12/20], Step [300/600], Loss: 0.2138\n",
            "Epoch [12/20], Step [400/600], Loss: 0.3336\n",
            "Epoch [12/20], Step [500/600], Loss: 0.2390\n",
            "Epoch [12/20], Step [600/600], Loss: 0.3155\n",
            "Epoch [13/20], Step [100/600], Loss: 0.3619\n",
            "Epoch [13/20], Step [200/600], Loss: 0.1796\n",
            "Epoch [13/20], Step [300/600], Loss: 0.1669\n",
            "Epoch [13/20], Step [400/600], Loss: 0.1485\n",
            "Epoch [13/20], Step [500/600], Loss: 0.2307\n",
            "Epoch [13/20], Step [600/600], Loss: 0.3110\n",
            "Epoch [14/20], Step [100/600], Loss: 0.2494\n",
            "Epoch [14/20], Step [200/600], Loss: 0.1803\n",
            "Epoch [14/20], Step [300/600], Loss: 0.2816\n",
            "Epoch [14/20], Step [400/600], Loss: 0.1316\n",
            "Epoch [14/20], Step [500/600], Loss: 0.2424\n",
            "Epoch [14/20], Step [600/600], Loss: 0.2010\n",
            "Epoch [15/20], Step [100/600], Loss: 0.3079\n",
            "Epoch [15/20], Step [200/600], Loss: 0.3545\n",
            "Epoch [15/20], Step [300/600], Loss: 0.2629\n",
            "Epoch [15/20], Step [400/600], Loss: 0.3421\n",
            "Epoch [15/20], Step [500/600], Loss: 0.3545\n",
            "Epoch [15/20], Step [600/600], Loss: 0.3504\n",
            "Epoch [16/20], Step [100/600], Loss: 0.3434\n",
            "Epoch [16/20], Step [200/600], Loss: 0.3215\n",
            "Epoch [16/20], Step [300/600], Loss: 0.2485\n",
            "Epoch [16/20], Step [400/600], Loss: 0.3807\n",
            "Epoch [16/20], Step [500/600], Loss: 0.1943\n",
            "Epoch [16/20], Step [600/600], Loss: 0.2480\n",
            "Epoch [17/20], Step [100/600], Loss: 0.2003\n",
            "Epoch [17/20], Step [200/600], Loss: 0.2679\n",
            "Epoch [17/20], Step [300/600], Loss: 0.1642\n",
            "Epoch [17/20], Step [400/600], Loss: 0.2616\n",
            "Epoch [17/20], Step [500/600], Loss: 0.3267\n",
            "Epoch [17/20], Step [600/600], Loss: 0.1739\n",
            "Epoch [18/20], Step [100/600], Loss: 0.2539\n",
            "Epoch [18/20], Step [200/600], Loss: 0.1916\n",
            "Epoch [18/20], Step [300/600], Loss: 0.3827\n",
            "Epoch [18/20], Step [400/600], Loss: 0.2705\n",
            "Epoch [18/20], Step [500/600], Loss: 0.1986\n",
            "Epoch [18/20], Step [600/600], Loss: 0.2856\n",
            "Epoch [19/20], Step [100/600], Loss: 0.3372\n",
            "Epoch [19/20], Step [200/600], Loss: 0.2674\n",
            "Epoch [19/20], Step [300/600], Loss: 0.2935\n",
            "Epoch [19/20], Step [400/600], Loss: 0.2690\n",
            "Epoch [19/20], Step [500/600], Loss: 0.3108\n",
            "Epoch [19/20], Step [600/600], Loss: 0.2560\n",
            "Epoch [20/20], Step [100/600], Loss: 0.3285\n",
            "Epoch [20/20], Step [200/600], Loss: 0.2105\n",
            "Epoch [20/20], Step [300/600], Loss: 0.1944\n",
            "Epoch [20/20], Step [400/600], Loss: 0.2478\n",
            "Epoch [20/20], Step [500/600], Loss: 0.3880\n",
            "Epoch [20/20], Step [600/600], Loss: 0.2905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(Network2Model, test_loader, device, writer3, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuEn4j4uTCmU",
        "outputId": "88a4cc89-f155-46a5-903f-5167b7c0d4ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 91.11 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing Results Network3"
      ],
      "metadata": {
        "id": "VQXTT9EXTHEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer3 = torch.optim.Adam(Network3Model.parameters(), lr=learning_rate)\n",
        "train_model(Network3Model, criterion, optimizer3, train_loader, device, num_epochs, writer4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rfdEdkPTLy7",
        "outputId": "541ac7f5-e9f9-4fc2-da63-3641954f5e9b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [100/600], Loss: 0.9544\n",
            "Epoch [1/20], Step [200/600], Loss: 0.6230\n",
            "Epoch [1/20], Step [300/600], Loss: 0.4956\n",
            "Epoch [1/20], Step [400/600], Loss: 0.3465\n",
            "Epoch [1/20], Step [500/600], Loss: 0.4935\n",
            "Epoch [1/20], Step [600/600], Loss: 0.3569\n",
            "Epoch [2/20], Step [100/600], Loss: 0.2480\n",
            "Epoch [2/20], Step [200/600], Loss: 0.2577\n",
            "Epoch [2/20], Step [300/600], Loss: 0.3242\n",
            "Epoch [2/20], Step [400/600], Loss: 0.2145\n",
            "Epoch [2/20], Step [500/600], Loss: 0.3685\n",
            "Epoch [2/20], Step [600/600], Loss: 0.3341\n",
            "Epoch [3/20], Step [100/600], Loss: 0.1958\n",
            "Epoch [3/20], Step [200/600], Loss: 0.4065\n",
            "Epoch [3/20], Step [300/600], Loss: 0.2455\n",
            "Epoch [3/20], Step [400/600], Loss: 0.3371\n",
            "Epoch [3/20], Step [500/600], Loss: 0.1141\n",
            "Epoch [3/20], Step [600/600], Loss: 0.2533\n",
            "Epoch [4/20], Step [100/600], Loss: 0.3278\n",
            "Epoch [4/20], Step [200/600], Loss: 0.2749\n",
            "Epoch [4/20], Step [300/600], Loss: 0.3005\n",
            "Epoch [4/20], Step [400/600], Loss: 0.1716\n",
            "Epoch [4/20], Step [500/600], Loss: 0.1317\n",
            "Epoch [4/20], Step [600/600], Loss: 0.3326\n",
            "Epoch [5/20], Step [100/600], Loss: 0.2083\n",
            "Epoch [5/20], Step [200/600], Loss: 0.2422\n",
            "Epoch [5/20], Step [300/600], Loss: 0.2114\n",
            "Epoch [5/20], Step [400/600], Loss: 0.3077\n",
            "Epoch [5/20], Step [500/600], Loss: 0.3065\n",
            "Epoch [5/20], Step [600/600], Loss: 0.2416\n",
            "Epoch [6/20], Step [100/600], Loss: 0.3393\n",
            "Epoch [6/20], Step [200/600], Loss: 0.3050\n",
            "Epoch [6/20], Step [300/600], Loss: 0.3030\n",
            "Epoch [6/20], Step [400/600], Loss: 0.2182\n",
            "Epoch [6/20], Step [500/600], Loss: 0.2075\n",
            "Epoch [6/20], Step [600/600], Loss: 0.1603\n",
            "Epoch [7/20], Step [100/600], Loss: 0.1823\n",
            "Epoch [7/20], Step [200/600], Loss: 0.2957\n",
            "Epoch [7/20], Step [300/600], Loss: 0.2274\n",
            "Epoch [7/20], Step [400/600], Loss: 0.4507\n",
            "Epoch [7/20], Step [500/600], Loss: 0.2825\n",
            "Epoch [7/20], Step [600/600], Loss: 0.3531\n",
            "Epoch [8/20], Step [100/600], Loss: 0.4416\n",
            "Epoch [8/20], Step [200/600], Loss: 0.1720\n",
            "Epoch [8/20], Step [300/600], Loss: 0.2441\n",
            "Epoch [8/20], Step [400/600], Loss: 0.4426\n",
            "Epoch [8/20], Step [500/600], Loss: 0.2917\n",
            "Epoch [8/20], Step [600/600], Loss: 0.2955\n",
            "Epoch [9/20], Step [100/600], Loss: 0.4686\n",
            "Epoch [9/20], Step [200/600], Loss: 0.2132\n",
            "Epoch [9/20], Step [300/600], Loss: 0.1921\n",
            "Epoch [9/20], Step [400/600], Loss: 0.2932\n",
            "Epoch [9/20], Step [500/600], Loss: 0.2832\n",
            "Epoch [9/20], Step [600/600], Loss: 0.2562\n",
            "Epoch [10/20], Step [100/600], Loss: 0.3201\n",
            "Epoch [10/20], Step [200/600], Loss: 0.2976\n",
            "Epoch [10/20], Step [300/600], Loss: 0.2957\n",
            "Epoch [10/20], Step [400/600], Loss: 0.1939\n",
            "Epoch [10/20], Step [500/600], Loss: 0.5239\n",
            "Epoch [10/20], Step [600/600], Loss: 0.3392\n",
            "Epoch [11/20], Step [100/600], Loss: 0.2830\n",
            "Epoch [11/20], Step [200/600], Loss: 0.5567\n",
            "Epoch [11/20], Step [300/600], Loss: 0.2696\n",
            "Epoch [11/20], Step [400/600], Loss: 0.1967\n",
            "Epoch [11/20], Step [500/600], Loss: 0.2869\n",
            "Epoch [11/20], Step [600/600], Loss: 0.1911\n",
            "Epoch [12/20], Step [100/600], Loss: 0.1813\n",
            "Epoch [12/20], Step [200/600], Loss: 0.3097\n",
            "Epoch [12/20], Step [300/600], Loss: 0.4219\n",
            "Epoch [12/20], Step [400/600], Loss: 0.2316\n",
            "Epoch [12/20], Step [500/600], Loss: 0.1959\n",
            "Epoch [12/20], Step [600/600], Loss: 0.2689\n",
            "Epoch [13/20], Step [100/600], Loss: 0.2807\n",
            "Epoch [13/20], Step [200/600], Loss: 0.1831\n",
            "Epoch [13/20], Step [300/600], Loss: 0.3327\n",
            "Epoch [13/20], Step [400/600], Loss: 0.4816\n",
            "Epoch [13/20], Step [500/600], Loss: 0.3115\n",
            "Epoch [13/20], Step [600/600], Loss: 0.3039\n",
            "Epoch [14/20], Step [100/600], Loss: 0.3145\n",
            "Epoch [14/20], Step [200/600], Loss: 0.3317\n",
            "Epoch [14/20], Step [300/600], Loss: 0.3049\n",
            "Epoch [14/20], Step [400/600], Loss: 0.3993\n",
            "Epoch [14/20], Step [500/600], Loss: 0.2484\n",
            "Epoch [14/20], Step [600/600], Loss: 0.2885\n",
            "Epoch [15/20], Step [100/600], Loss: 0.2587\n",
            "Epoch [15/20], Step [200/600], Loss: 0.3034\n",
            "Epoch [15/20], Step [300/600], Loss: 0.2242\n",
            "Epoch [15/20], Step [400/600], Loss: 0.2767\n",
            "Epoch [15/20], Step [500/600], Loss: 0.2163\n",
            "Epoch [15/20], Step [600/600], Loss: 0.3093\n",
            "Epoch [16/20], Step [100/600], Loss: 0.3203\n",
            "Epoch [16/20], Step [200/600], Loss: 0.3169\n",
            "Epoch [16/20], Step [300/600], Loss: 0.1892\n",
            "Epoch [16/20], Step [400/600], Loss: 0.3925\n",
            "Epoch [16/20], Step [500/600], Loss: 0.2859\n",
            "Epoch [16/20], Step [600/600], Loss: 0.3290\n",
            "Epoch [17/20], Step [100/600], Loss: 0.2723\n",
            "Epoch [17/20], Step [200/600], Loss: 0.3122\n",
            "Epoch [17/20], Step [300/600], Loss: 0.1118\n",
            "Epoch [17/20], Step [400/600], Loss: 0.1771\n",
            "Epoch [17/20], Step [500/600], Loss: 0.2537\n",
            "Epoch [17/20], Step [600/600], Loss: 0.4137\n",
            "Epoch [18/20], Step [100/600], Loss: 0.2973\n",
            "Epoch [18/20], Step [200/600], Loss: 0.2805\n",
            "Epoch [18/20], Step [300/600], Loss: 0.4464\n",
            "Epoch [18/20], Step [400/600], Loss: 0.2591\n",
            "Epoch [18/20], Step [500/600], Loss: 0.3056\n",
            "Epoch [18/20], Step [600/600], Loss: 0.2822\n",
            "Epoch [19/20], Step [100/600], Loss: 0.2285\n",
            "Epoch [19/20], Step [200/600], Loss: 0.1474\n",
            "Epoch [19/20], Step [300/600], Loss: 0.2358\n",
            "Epoch [19/20], Step [400/600], Loss: 0.2391\n",
            "Epoch [19/20], Step [500/600], Loss: 0.2368\n",
            "Epoch [19/20], Step [600/600], Loss: 0.1330\n",
            "Epoch [20/20], Step [100/600], Loss: 0.2230\n",
            "Epoch [20/20], Step [200/600], Loss: 0.1789\n",
            "Epoch [20/20], Step [300/600], Loss: 0.1753\n",
            "Epoch [20/20], Step [400/600], Loss: 0.1716\n",
            "Epoch [20/20], Step [500/600], Loss: 0.1468\n",
            "Epoch [20/20], Step [600/600], Loss: 0.2571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(Network3Model, test_loader, device, writer4, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SLq4HwLTTkZ",
        "outputId": "9a4d65bc-7a41-455e-93dc-4415ac6d4260"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 92.57 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "uctTsewbTZqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a comparative analysis of three neural network models designed for a classification task, it becomes apparent that the architecture complexity varies significantly across the models. The first model is a 4-layer network with intermediate layers composed of 20, 50, and 20 neurons, respectively, followed by an output layer with a softmax activation function. The second model expands upon this with a 6-layer configuration, including neuron counts of 10, 20, 30, 20, and 10 in the intermediate layers, also concluding with a softmax output. The third model mirrors this 6-layer approach but opts for a more complex neuron arrangement: 10, 40, 70, 40, and 10 neurons across its intermediate layers, leading to a softmax output.\n",
        "\n",
        "Upon completion of 20 training epochs, set with a learning rate of 0.01, the models yielded the following accuracy rates on a test set of 10,000 images: Network1 reached the highest accuracy of 95.83%, whereas Network2 and Network3 recorded lower accuracies of 91.11% and 92.57%, respectively. Based on these figures, Network1 emerges as the model with the least error in validation.\n",
        "\n",
        "The duration of training for each model was not explicitly provided; however, it's generally observed that networks with fewer parameters and less complexity tend to train faster. In this scenario, one would expect Network1 to train faster than the others due to its simpler 4-layer architecture. The more extensive and deeper networks (Network2 and Network3) likely required more time for training, with Network3 potentially taking the longest given its greater number of neurons, despite both having six layers."
      ],
      "metadata": {
        "id": "pqukUuUMTf7m"
      }
    }
  ]
}